{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50463b40",
   "metadata": {},
   "source": [
    "### Credits:\n",
    "\n",
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png\"><br />\n",
    "\n",
    "This notebook is created by Zhuo Chen based on the notebooks created by [Nathan Kelber](http://nkelber.com) under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "For questions/comments/improvements, email zhuo.chen@ithaka.org or nathan.kelber@ithaka.org<br />\n",
    "\n",
    "Reused and modified for internal use at Università Cattolica del Sacro Cuore di Milano, by Deborah Grbac, email deborah.grbac@unicatt.it and Valentina Schiariti, email valentina.schiariti-collaboratore@unicatt.it, released under CC BY License.\n",
    "\n",
    "This repository is founded on **Constellate notebooks**. The original Jupyter notebooks repository was designed by the educators at **ITHAKA's Constellate project**. The project was sunset on July 1, 2025. This current repository uses and resuses Constellate notebooks as Open Educational Resources (OER), free for re-use under a Creative Commons CC BY License.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f72056",
   "metadata": {},
   "source": [
    "\n",
    "# Tokenizers\n",
    "\n",
    "**Description:**\n",
    "This notebook focuses on the basic concepts surrounding tokenization. It includes material on the following concepts:\n",
    "\n",
    "* Word segmentation\n",
    "* n-grams\n",
    "* Stemming\n",
    "* Lemmatization\n",
    "* Tokenizers\n",
    "\n",
    "**Libraries Used:**\n",
    "* urllib.request\n",
    "* NLTK\n",
    "* spaCy\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7737942",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Tokenization and words\n",
    "\n",
    "**Tokenization** is the process of segmenting text into smaller units, called **tokens**, which may be sentences, words, or sub-word chunks. It is typically the first step in a Natural Language Processing (NLP) pipeline and can be carried out by a variety of tokenizers, each reflecting different design choices.\n",
    "\n",
    "A simple approach to word tokenization splits text on whitespace and punctuation.\n",
    "\n",
    "> Now that summer's here, we're going to visit the beach at Lake Michigan and eat ice cream.\n",
    "\n",
    "By splitting on whitespace only, we would get in this case 17 words: \n",
    "\n",
    "> Now, that, summer's, here, we're, going, to, visit, the, beach, at, Lake, Michigan, and, eat, ice, cream.\n",
    "\n",
    "However, this raises questions: should “Lake Michigan” count as one token or two? Is “we’re” one word or two? Should “going” be treated differently from “go” or “went”?  \n",
    "\n",
    "These challenges reveal that even the seemingly straightforward **concept of a “word”** becomes complicated when formalized for computational analysis. This is why more advanced tokenization methods, such as Byte-Pair Encoding (BPE), WordPiece, and SentencePiece, were developed to address these issues in modern language models like BERT and GPT. \n",
    "\n",
    "We will look at a few examples of traditional tokenizers with a goal of gathering tokens into one-, two-, and three-word constructions. The general name for these is **n-grams**.\n",
    "\n",
    "An **n-gram** is a **sequence of n items from a given sample of text or speech**. Most often, this refers to a sequence of words, but it can also be used to analyze text at the level of syllables, letters, or phonemes. N-grams are often described by their length. For example, word n-grams might include:\n",
    "\n",
    "* stock (a 1-gram, or **unigram**)\n",
    "* vegetable stock (a 2-gram, or **bigram**)\n",
    "* homemade vegetable stock (a 3-gram, or **trigram**)\n",
    "\n",
    "Analyzing text through n-grams allows us to capture meaning that extends beyond single words. By looking only at unigrams would not be able, for example, to differentiate between the \"stock\" in \"stock market\" and \"vegetable stock.\" By including bigrams and trigrams in our analysis, we are able to look at concepts that extend across multiple words. \n",
    "\n",
    "One of the most popular examples of text analysis with n-grams is the [Google N-Gram Viewer](https://books.google.com/ngrams)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0303d2",
   "metadata": {},
   "source": [
    "## Creating your own basic tokenizer\n",
    "\n",
    "As explained, the most intuitive way to turn a text into tokens is by  on whitespace and punctuation. It is possible to create your own basic tokenizer by using Python string methods. \n",
    "\n",
    "The following example uses the `.split()` method to gather unigrams. We will be using an extract from Shakespeare's Othello, provided at this [link](https://www.folger.edu/explore/shakespeares-works/othello/read/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84d6ada5-eb11-41c9-9f9a-aabfcbf1e875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/othello_TXT_FolgerShakespeare.txt',\n",
       " <http.client.HTTPMessage at 0x20474e2f610>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "# Step 1: Create a ./data folder if it doesn’t exist\n",
    "data_folder = Path(\"./data/\")\n",
    "data_folder.mkdir(exist_ok=True)\n",
    "\n",
    "# Step 2: Load the text file (after the person has placed it there)\n",
    "text_address = \"https://folger-main-site-assets.s3.amazonaws.com/uploads/2022/11/othello_TXT_FolgerShakespeare.txt\"\n",
    "text_name = './data/' + text_address.rsplit('/', 1)[-1]\n",
    "urllib.request.urlretrieve(text_address, text_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c992c34f-0cfd-4331-95a0-05f7d186892e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "text_path = data_folder / \"othello_TXT_FolgerShakespeare.txt\"\n",
    "\n",
    "if text_path.exists():\n",
    "    print(\"File loaded successfully!\")\n",
    "else:\n",
    "    print(\"File not found. Please download it and place it in the ./data/ folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee17ae0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Othello\n",
      "by William Shakespeare\n",
      "Edited by Barbara A. Mowat and Paul Werstine\n",
      "  with Michael Poston and Rebecca Niles\n",
      "Folger Shakespeare Library\n",
      "https://shakespeare.folger.edu/shakespeares-works/othello/\n",
      "Created on May 11, 2016, from FDT version 0.9.2.1\n",
      "\n",
      "Characters in the Play\n",
      "======================\n",
      "OTHELLO, a Moorish general in the Venetian army\n",
      "DESDEMONA, a Venetian lady\n",
      "BRABANTIO, a Venetian senator, father to Desdemona\n",
      "IAGO, Othello's standard-bearer, or \"ancient\"\n",
      "EMILIA, Iago's wife and Desdemona's attendant\n",
      "CASSIO, Othello's second-in-command, or lieutenant\n",
      "RODERIGO, a Venetian gentleman\n",
      "Duke of Venice\n",
      "Venetian gentlemen, kinsmen to Brabantio:\n",
      "  LODOVICO\n",
      "  GRATIANO\n",
      "Venetian senators\n",
      "MONTANO, an official in Cyprus\n",
      "BIANCA, a woman in Cyprus in love with Cassio\n",
      "Clown, a comic servant to Othello and Desdemona\n",
      "Gentlemen of Cyprus\n",
      "Sailors\n",
      "Servants, Attendants, Officers, Messengers, Herald, Musicians, Torchbearers.\n",
      "\n",
      "\n",
      "ACT 1\n",
      "=====\n",
      "\n",
      "Scene 1\n",
      "=======\n",
      "[Enter Roderigo and Iago.]\n",
      "\n",
      "\n",
      "RODERIGO\n",
      "Tush,\n"
     ]
    }
   ],
   "source": [
    "# Opening a file in read mode\n",
    "with open(text_path, \"r\") as f:\n",
    "    othello_text_extract = f.read(1000) #reading an extract of the text\n",
    "    print(othello_text_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a00864c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Othello\\nby William Shakespeare\\nEdited by Barbara A. Mowat and Paul Werstine\\n  with Michael Poston and Rebecca Niles\\nFolger Shakespeare Library\\nhttps://shakespeare.folger.edu/shakespeares-works/othello/\\nCreated on May 11, 2016, from FDT version 0.9.2.1\\n\\nCharacters in the Play\\n======================\\nOTHELLO, a Moorish general in the Venetian army\\nDESDEMONA, a Venetian lady\\nBRABANTIO, a Venetian senator, father to Desdemona\\nIAGO, Othello\\'s standard-bearer, or \"ancient\"\\nEMILIA, Iago\\'s wife and Desdemona\\'s attendant\\nCASSIO, Othello\\'s second-in-command, or lieutenant\\nRODERIGO, a Venetian gentleman\\nDuke of Venice\\nVenetian gentlemen, kinsmen to Brabantio:\\n  LODOVICO\\n  GRATIANO\\nVenetian senators\\nMONTANO, an official in Cyprus\\nBIANCA, a woman in Cyprus in love with Cassio\\nClown, a comic servant to Othello and Desdemona\\nGentlemen of Cyprus\\nSailors\\nServants, Attendants, Officers, Messengers, Herald, Musicians, Torchbearers.\\n\\n\\nACT 1\\n=====\\n\\nScene 1\\n=======\\n[Enter Roderigo and Iago.]\\n\\n\\nRODERIGO\\nTush,'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the raw string version of our text\n",
    "othello_text_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "292a965f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Othello',\n",
       " 'by',\n",
       " 'William',\n",
       " 'Shakespeare',\n",
       " 'Edited',\n",
       " 'by',\n",
       " 'Barbara',\n",
       " 'A.',\n",
       " 'Mowat',\n",
       " 'and',\n",
       " 'Paul',\n",
       " 'Werstine',\n",
       " 'with',\n",
       " 'Michael',\n",
       " 'Poston',\n",
       " 'and',\n",
       " 'Rebecca',\n",
       " 'Niles',\n",
       " 'Folger',\n",
       " 'Shakespeare',\n",
       " 'Library',\n",
       " 'https://shakespeare.folger.edu/shakespeares-works/othello/',\n",
       " 'Created',\n",
       " 'on',\n",
       " 'May',\n",
       " '11,',\n",
       " '2016,',\n",
       " 'from',\n",
       " 'FDT',\n",
       " 'version',\n",
       " '0.9.2.1',\n",
       " 'Characters',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Play',\n",
       " '======================',\n",
       " 'OTHELLO,',\n",
       " 'a',\n",
       " 'Moorish',\n",
       " 'general',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Venetian',\n",
       " 'army',\n",
       " 'DESDEMONA,',\n",
       " 'a',\n",
       " 'Venetian',\n",
       " 'lady',\n",
       " 'BRABANTIO,',\n",
       " 'a',\n",
       " 'Venetian',\n",
       " 'senator,',\n",
       " 'father',\n",
       " 'to',\n",
       " 'Desdemona',\n",
       " 'IAGO,',\n",
       " \"Othello's\",\n",
       " 'standard-bearer,',\n",
       " 'or',\n",
       " '\"ancient\"',\n",
       " 'EMILIA,',\n",
       " \"Iago's\",\n",
       " 'wife',\n",
       " 'and',\n",
       " \"Desdemona's\",\n",
       " 'attendant',\n",
       " 'CASSIO,',\n",
       " \"Othello's\",\n",
       " 'second-in-command,',\n",
       " 'or',\n",
       " 'lieutenant',\n",
       " 'RODERIGO,',\n",
       " 'a',\n",
       " 'Venetian',\n",
       " 'gentleman',\n",
       " 'Duke',\n",
       " 'of',\n",
       " 'Venice',\n",
       " 'Venetian',\n",
       " 'gentlemen,',\n",
       " 'kinsmen',\n",
       " 'to',\n",
       " 'Brabantio:',\n",
       " 'LODOVICO',\n",
       " 'GRATIANO',\n",
       " 'Venetian',\n",
       " 'senators',\n",
       " 'MONTANO,',\n",
       " 'an',\n",
       " 'official',\n",
       " 'in',\n",
       " 'Cyprus',\n",
       " 'BIANCA,',\n",
       " 'a',\n",
       " 'woman',\n",
       " 'in',\n",
       " 'Cyprus',\n",
       " 'in',\n",
       " 'love',\n",
       " 'with',\n",
       " 'Cassio',\n",
       " 'Clown,',\n",
       " 'a',\n",
       " 'comic',\n",
       " 'servant',\n",
       " 'to',\n",
       " 'Othello',\n",
       " 'and',\n",
       " 'Desdemona',\n",
       " 'Gentlemen',\n",
       " 'of',\n",
       " 'Cyprus',\n",
       " 'Sailors',\n",
       " 'Servants,',\n",
       " 'Attendants,',\n",
       " 'Officers,',\n",
       " 'Messengers,',\n",
       " 'Herald,',\n",
       " 'Musicians,',\n",
       " 'Torchbearers.',\n",
       " 'ACT',\n",
       " '1',\n",
       " '=====',\n",
       " 'Scene',\n",
       " '1',\n",
       " '=======',\n",
       " '[Enter',\n",
       " 'Roderigo',\n",
       " 'and',\n",
       " 'Iago.]',\n",
       " 'RODERIGO',\n",
       " 'Tush,']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the text string into a list of strings\n",
    "extract_tokenized_list =  othello_text_extract.split()\n",
    "list(extract_tokenized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a168363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up the tokens\n",
    "unigrams = []\n",
    "\n",
    "for token in extract_tokenized_list:\n",
    "    token = token.lower() # lowercase tokens\n",
    "    token = token.replace('.', '') # remove periods\n",
    "    token = token.replace('!', '') # remove exclamation points\n",
    "    token = token.replace('?', '') # remove question marks\n",
    "    unigrams.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb63d832",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['othello',\n",
       " 'by',\n",
       " 'william',\n",
       " 'shakespeare',\n",
       " 'edited',\n",
       " 'by',\n",
       " 'barbara',\n",
       " 'a',\n",
       " 'mowat',\n",
       " 'and',\n",
       " 'paul',\n",
       " 'werstine',\n",
       " 'with',\n",
       " 'michael',\n",
       " 'poston',\n",
       " 'and',\n",
       " 'rebecca',\n",
       " 'niles',\n",
       " 'folger',\n",
       " 'shakespeare',\n",
       " 'library',\n",
       " 'https://shakespearefolgeredu/shakespeares-works/othello/',\n",
       " 'created',\n",
       " 'on',\n",
       " 'may',\n",
       " '11,',\n",
       " '2016,',\n",
       " 'from',\n",
       " 'fdt',\n",
       " 'version',\n",
       " '0921',\n",
       " 'characters',\n",
       " 'in',\n",
       " 'the',\n",
       " 'play',\n",
       " '======================',\n",
       " 'othello,',\n",
       " 'a',\n",
       " 'moorish',\n",
       " 'general',\n",
       " 'in',\n",
       " 'the',\n",
       " 'venetian',\n",
       " 'army',\n",
       " 'desdemona,',\n",
       " 'a',\n",
       " 'venetian',\n",
       " 'lady',\n",
       " 'brabantio,',\n",
       " 'a',\n",
       " 'venetian',\n",
       " 'senator,',\n",
       " 'father',\n",
       " 'to',\n",
       " 'desdemona',\n",
       " 'iago,',\n",
       " \"othello's\",\n",
       " 'standard-bearer,',\n",
       " 'or',\n",
       " '\"ancient\"',\n",
       " 'emilia,',\n",
       " \"iago's\",\n",
       " 'wife',\n",
       " 'and',\n",
       " \"desdemona's\",\n",
       " 'attendant',\n",
       " 'cassio,',\n",
       " \"othello's\",\n",
       " 'second-in-command,',\n",
       " 'or',\n",
       " 'lieutenant',\n",
       " 'roderigo,',\n",
       " 'a',\n",
       " 'venetian',\n",
       " 'gentleman',\n",
       " 'duke',\n",
       " 'of',\n",
       " 'venice',\n",
       " 'venetian',\n",
       " 'gentlemen,',\n",
       " 'kinsmen',\n",
       " 'to',\n",
       " 'brabantio:',\n",
       " 'lodovico',\n",
       " 'gratiano',\n",
       " 'venetian',\n",
       " 'senators',\n",
       " 'montano,',\n",
       " 'an',\n",
       " 'official',\n",
       " 'in',\n",
       " 'cyprus',\n",
       " 'bianca,',\n",
       " 'a',\n",
       " 'woman',\n",
       " 'in',\n",
       " 'cyprus',\n",
       " 'in',\n",
       " 'love',\n",
       " 'with',\n",
       " 'cassio',\n",
       " 'clown,',\n",
       " 'a',\n",
       " 'comic',\n",
       " 'servant',\n",
       " 'to',\n",
       " 'othello',\n",
       " 'and',\n",
       " 'desdemona',\n",
       " 'gentlemen',\n",
       " 'of',\n",
       " 'cyprus',\n",
       " 'sailors',\n",
       " 'servants,',\n",
       " 'attendants,',\n",
       " 'officers,',\n",
       " 'messengers,',\n",
       " 'herald,',\n",
       " 'musicians,',\n",
       " 'torchbearers',\n",
       " 'act',\n",
       " '1',\n",
       " '=====',\n",
       " 'scene',\n",
       " '1',\n",
       " '=======',\n",
       " '[enter',\n",
       " 'roderigo',\n",
       " 'and',\n",
       " 'iago]',\n",
       " 'roderigo',\n",
       " 'tush,']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview the unigrams\n",
    "list(unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b89861e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'a': 7, 'venetian': 6, 'and': 5, 'in': 5, 'to': 3, 'cyprus': 3, 'othello': 2, 'by': 2, 'shakespeare': 2, 'with': 2, 'the': 2, 'desdemona': 2, \"othello's\": 2, 'or': 2, 'of': 2, '1': 2, 'roderigo': 2, 'william': 1, 'edited': 1, 'barbara': 1, 'mowat': 1, 'paul': 1, 'werstine': 1, 'michael': 1, 'poston': 1, 'rebecca': 1, 'niles': 1, 'folger': 1, 'library': 1, 'https://shakespearefolgeredu/shakespeares-works/othello/': 1, 'created': 1, 'on': 1, 'may': 1, '11,': 1, '2016,': 1, 'from': 1, 'fdt': 1, 'version': 1, '0921': 1, 'characters': 1, 'play': 1, '======================': 1, 'othello,': 1, 'moorish': 1, 'general': 1, 'army': 1, 'desdemona,': 1, 'lady': 1, 'brabantio,': 1, 'senator,': 1, 'father': 1, 'iago,': 1, 'standard-bearer,': 1, '\"ancient\"': 1, 'emilia,': 1, \"iago's\": 1, 'wife': 1, \"desdemona's\": 1, 'attendant': 1, 'cassio,': 1, 'second-in-command,': 1, 'lieutenant': 1, 'roderigo,': 1, 'gentleman': 1, 'duke': 1, 'venice': 1, 'gentlemen,': 1, 'kinsmen': 1, 'brabantio:': 1, 'lodovico': 1, 'gratiano': 1, 'senators': 1, 'montano,': 1, 'an': 1, 'official': 1, 'bianca,': 1, 'woman': 1, 'love': 1, 'cassio': 1, 'clown,': 1, 'comic': 1, 'servant': 1, 'gentlemen': 1, 'sailors': 1, 'servants,': 1, 'attendants,': 1, 'officers,': 1, 'messengers,': 1, 'herald,': 1, 'musicians,': 1, 'torchbearers': 1, 'act': 1, '=====': 1, 'scene': 1, '=======': 1, '[enter': 1, 'iago]': 1, 'tush,': 1})\n"
     ]
    }
   ],
   "source": [
    "# Count up the tokens using a Counter() object\n",
    "from collections import Counter\n",
    "word_counts = Counter(unigrams)\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74698aac-dd0e-4207-9847-3fdb5f87ca4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 805), ('and', 781), ('the', 701), ('to', 575), ('of', 443), ('a', 436), ('my', 429), ('you', 418), ('that', 355), ('in', 332), ('is', 303), ('not', 299), ('iago', 298), ('othello', 289), ('it', 280), ('me', 241), ('for', 226), ('with', 224), ('but', 215), ('be', 213), ('do', 212), ('your', 210), ('this', 207), ('have', 204), ('he', 201), ('desdemona', 194), ('her', 193), ('cassio', 189), ('his', 167), ('as', 163), ('what', 159), ('him', 151), ('she', 150), ('will', 145), ('if', 140), ('thou', 140), ('so', 140), ('by', 113), ('on', 108), ('emilia', 103), ('are', 102), ('shall', 96), ('am', 90), (\"'tis\", 88), ('or', 87), ('all', 86), ('roderigo', 83), ('good', 83), (\"'t\", 82), ('would', 81)]\n"
     ]
    }
   ],
   "source": [
    "#Applying to the full text\n",
    "\n",
    "#Step 1: open the text in read mode and create file variable\n",
    "with open(text_path, \"r\") as f:\n",
    "    othello_text_full = f.read()\n",
    "\n",
    "#Step 2: split the text on whitespace    \n",
    "full_tokenized_list =  othello_text_full.split()\n",
    "\n",
    "#Step 3: text normalization\n",
    "full_unigrams = []\n",
    "\n",
    "for token in full_tokenized_list:\n",
    "    token = token.lower() # lowercase tokens\n",
    "    token = token.replace('.', '') # remove periods\n",
    "    token = token.replace('!', '') # remove exclamation points\n",
    "    token = token.replace('?', '') # remove question marks\n",
    "    full_unigrams.append(token)\n",
    "\n",
    "#Step 4: Unigram word frequency counting\n",
    "from collections import Counter\n",
    "full_word_counts = Counter(full_unigrams)\n",
    "print(full_word_counts.most_common(50)) #print the first 50 as example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5998030",
   "metadata": {},
   "source": [
    "## NLTK\n",
    "\n",
    "While writing your own tokenizer may allow you to create highly customized results, it is easier and more often more effective to use **existing tokenizers** offered in packages such as the **Natural Language Toolkit (NLTK)** and **spaCy**. \n",
    "\n",
    "\n",
    "The NLTK library has multiple tokenizers available, each with its own specific advantages and disadvantages. \n",
    "\n",
    "### [Word Punctuation](https://www.nltk.org/_modules/nltk/tokenize/punkt.html)\n",
    "The word punctuation tokenizer splits on white spaces and splits out punctuation into separate tokens.\n",
    "\n",
    "### [Penn Treebank](https://www.nltk.org/_modules/nltk/tokenize/treebank.html)\n",
    "The Tree Bank tokenizer is the default tokenizer for NLTK. It features a variety of regular expressions for addressing punctuation such as contractions, quotes, parentheses, brackets, and dashes.\n",
    "\n",
    "### [Tweet](https://www.nltk.org/_modules/nltk/tokenize/casual.html#TweetTokenizer)\n",
    "The Twitter tokenizer is designed to work with Twitter and social media text. It uses regular expressions for addressing emoticons, phone numbers, URLs, Twitter usernames, and email addresses.\n",
    "\n",
    "### [Multi-Word Expression](https://www.nltk.org/_modules/nltk/tokenize/mwe.html)\n",
    "The MWETokenizer takes a \"string which has already been divided into tokens and retokenizes it, merging multi-word expressions into single tokens, using a lexicon of MWEs.\" The lexicon of Multi-Word Entities is constructed by the user. It can be constructed ad-hoc depended on the user's research interest or discovered through the use of techniques like part of speech tagging, collocation, and named entity recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7bd7206",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (2025.7.34)\n",
      "Requirement already satisfied: tqdm in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Utente\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Utente\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Import a variety of tokenizers\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import (TreebankWordTokenizer,\n",
    "                          word_tokenize,\n",
    "                          wordpunct_tokenize,\n",
    "                          TweetTokenizer,\n",
    "                          MWETokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9288b7c9-6843-4cae-a38c-adfc5b56e8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"Last weekend, I traveled to New York City and wrote about my experience on https://example.com — it was amazing! #TravelDiaries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2ccb5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python .split()\n",
      "['Last', 'weekend,', 'I', 'traveled', 'to', 'New', 'York', 'City', 'and', 'wrote', 'about', 'my', 'experience', 'on', 'https://example.com', '—', 'it', 'was', 'amazing!', '#TravelDiaries'] \n",
      "\n",
      "Wordpunct tokenizer\n",
      "['Last', 'weekend', ',', 'I', 'traveled', 'to', 'New', 'York', 'City', 'and', 'wrote', 'about', 'my', 'experience', 'on', 'https', '://', 'example', '.', 'com', '—', 'it', 'was', 'amazing', '!', '#', 'TravelDiaries'] \n",
      "\n",
      "Treebank Tokenizer\n",
      "['Last', 'weekend', ',', 'I', 'traveled', 'to', 'New', 'York', 'City', 'and', 'wrote', 'about', 'my', 'experience', 'on', 'https', ':', '//example.com', '—', 'it', 'was', 'amazing', '!', '#', 'TravelDiaries'] \n",
      "\n",
      "Tweet Tokenizer\n",
      "['Last', 'weekend', ',', 'I', 'traveled', 'to', 'New', 'York', 'City', 'and', 'wrote', 'about', 'my', 'experience', 'on', 'https://example.com', '—', 'it', 'was', 'amazing', '!', '#TravelDiaries'] \n",
      "\n",
      "MWE Tokenizer\n",
      "['Last', 'weekend', ',', 'I', 'traveled', 'to', 'New_York_City', 'and', 'wrote', 'about', 'my', 'experience', 'on', 'https', ':', '//example.com', '—', 'it', 'was', 'amazing', '!', '#', 'TravelDiaries']\n"
     ]
    }
   ],
   "source": [
    "# Python .split() tokenization\n",
    "split_tokens = string.split()\n",
    "print('Python .split()')\n",
    "print(split_tokens, '\\n')\n",
    "\n",
    "# Punctuation-based tokenization\n",
    "punct_tokens = wordpunct_tokenize(string)\n",
    "print('Wordpunct tokenizer')\n",
    "print(punct_tokens, '\\n')\n",
    "\n",
    "# Treebank Tokenizer\n",
    "treebank_tokens = TreebankWordTokenizer().tokenize(string)\n",
    "print('Treebank Tokenizer')\n",
    "print(treebank_tokens, '\\n')\n",
    "\n",
    "# TweetTokenizer\n",
    "tweet_tokens = TweetTokenizer().tokenize(string)\n",
    "print('Tweet Tokenizer')\n",
    "print(tweet_tokens, '\\n')\n",
    "\n",
    "# Multi-Word Expression Tokenizer\n",
    "tokenizer = MWETokenizer([('New', 'York','City')])\n",
    "MWE_tokens = tokenizer.tokenize(word_tokenize(string))\n",
    "print('MWE Tokenizer')\n",
    "print(MWE_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1e81ad",
   "metadata": {},
   "source": [
    "The tokenizer will generate a list of unigrams, but we still need to generate our bigrams and trigrams. We can simply pass the tokens into NLTK's bigrams and trigrams methods then store the results in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd81f814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams: \n",
      "  [('Last', 'weekend'), ('weekend', ','), (',', 'I'), ('I', 'traveled'), ('traveled', 'to'), ('to', 'New'), ('New', 'York'), ('York', 'City'), ('City', 'and'), ('and', 'wrote'), ('wrote', 'about'), ('about', 'my'), ('my', 'experience'), ('experience', 'on'), ('on', 'https'), ('https', ':'), (':', '//example.com'), ('//example.com', '—'), ('—', 'it'), ('it', 'was'), ('was', 'amazing'), ('amazing', '!'), ('!', '#'), ('#', 'TravelDiaries')] \n",
      "\n",
      "Trigrams: \n",
      ", [('Last', 'weekend', ','), ('weekend', ',', 'I'), (',', 'I', 'traveled'), ('I', 'traveled', 'to'), ('traveled', 'to', 'New'), ('to', 'New', 'York'), ('New', 'York', 'City'), ('York', 'City', 'and'), ('City', 'and', 'wrote'), ('and', 'wrote', 'about'), ('wrote', 'about', 'my'), ('about', 'my', 'experience'), ('my', 'experience', 'on'), ('experience', 'on', 'https'), ('on', 'https', ':'), ('https', ':', '//example.com'), (':', '//example.com', '—'), ('//example.com', '—', 'it'), ('—', 'it', 'was'), ('it', 'was', 'amazing'), ('was', 'amazing', '!'), ('amazing', '!', '#'), ('!', '#', 'TravelDiaries')]\n"
     ]
    }
   ],
   "source": [
    "# Creating our bigrams and trigrams\n",
    "bigrams = list(nltk.bigrams(treebank_tokens))\n",
    "trigrams = list(nltk.trigrams(treebank_tokens))\n",
    "\n",
    "print('Bigrams: \\n ', bigrams, '\\n')\n",
    "    \n",
    "print('Trigrams: \\n,', trigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d9d630",
   "metadata": {},
   "source": [
    "The NLTK bigrams and trigrams method creates a list of bigrams that are tuples. If we want them to be strings, then we would need to access each index of the tuple and create a string out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e63a282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definitions for Converting NLTK tuples into strings\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def convert_tuple_bigrams(tuples_to_convert):\n",
    "    \"\"\"Converts NLTK tuples into bigram strings\"\"\"\n",
    "    string_grams = []\n",
    "    for tuple_grams in tuples_to_convert:\n",
    "        first_word = tuple_grams[0]\n",
    "        second_word = tuple_grams[1]\n",
    "        gram_string = f'{first_word} {second_word}'\n",
    "        string_grams.append(gram_string)\n",
    "    return string_grams\n",
    "\n",
    "def convert_tuple_trigrams(tuples_to_convert):\n",
    "    \"\"\"Converts NLTK tuples into trigram strings\"\"\"\n",
    "    string_grams = []\n",
    "    for tuple_grams in tuples_to_convert:\n",
    "        first_word = tuple_grams[0]\n",
    "        second_word = tuple_grams[1]\n",
    "        third_word = tuple_grams[2]\n",
    "        gram_string = f'{first_word} {second_word} {third_word}'\n",
    "        string_grams.append(gram_string)\n",
    "    return string_grams\n",
    "\n",
    "def convert_strings_to_counts(string_grams):\n",
    "    \"\"\"Converts a Counter of n-grams into a dictionary\"\"\"\n",
    "    counter_of_grams = Counter(string_grams)\n",
    "    dict_of_grams = dict(counter_of_grams)\n",
    "    return dict_of_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a055f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams as a dictionary of counts\n",
      "{'Last weekend': 1, 'weekend ,': 1, ', I': 1, 'I traveled': 1, 'traveled to': 1, 'to New': 1, 'New York': 1, 'York City': 1, 'City and': 1, 'and wrote': 1, 'wrote about': 1, 'about my': 1, 'my experience': 1, 'experience on': 1, 'on https': 1, 'https :': 1, ': //example.com': 1, '//example.com —': 1, '— it': 1, 'it was': 1, 'was amazing': 1, 'amazing !': 1, '! #': 1, '# TravelDiaries': 1} \n",
      "\n",
      "Trigrams as a dictionary of counts\n",
      "{'Last weekend ,': 1, 'weekend , I': 1, ', I traveled': 1, 'I traveled to': 1, 'traveled to New': 1, 'to New York': 1, 'New York City': 1, 'York City and': 1, 'City and wrote': 1, 'and wrote about': 1, 'wrote about my': 1, 'about my experience': 1, 'my experience on': 1, 'experience on https': 1, 'on https :': 1, 'https : //example.com': 1, ': //example.com —': 1, '//example.com — it': 1, '— it was': 1, 'it was amazing': 1, 'was amazing !': 1, 'amazing ! #': 1, '! # TravelDiaries': 1}\n"
     ]
    }
   ],
   "source": [
    "# Converting the tuples\n",
    "string_bigrams = convert_tuple_bigrams(bigrams)\n",
    "bigramCount = convert_strings_to_counts(string_bigrams)\n",
    "\n",
    "print('Bigrams as a dictionary of counts')\n",
    "print(bigramCount, '\\n')\n",
    "\n",
    "string_trigrams = convert_tuple_trigrams(trigrams)\n",
    "trigramCount = convert_strings_to_counts(string_trigrams)\n",
    "\n",
    "print('Trigrams as a dictionary of counts')\n",
    "print(trigramCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca9dfd6",
   "metadata": {},
   "source": [
    "### Stemmer and Speech Tagging\n",
    "\n",
    "the NLTK library can also be used for Stemming and Speech tagging: \n",
    "\n",
    "A stemmer removes the endings of words to obtain their base form. The idea is to group together related words that share the same core meaning, regardless of their tense or number (singular or plural).\n",
    "\n",
    "* ducks -> duck\n",
    "* flown -> fly\n",
    "\n",
    "This process has its limits: stemmatization relies on string rules, not grammar or meaning. It doesn’t “know” what a word means or how it’s used in a sentence. This can cause over-stemming (different words are merged incorrectly, e.g., organize, organization → organ) and under-stemming (similar words are not reduced to the same stem). At the same time, it can be difficult to use for morphologically rich languages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "783f4743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "flown\n"
     ]
    }
   ],
   "source": [
    "# Snowball stemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "unstemmed_token1 = 'running'\n",
    "unstemmed_token2 = 'flown'\n",
    "\n",
    "stemmed_token1 = stemmer.stem(unstemmed_token1)\n",
    "stemmed_token2 = stemmer.stem(unstemmed_token2)\n",
    "\n",
    "print(stemmed_token1)\n",
    "print(stemmed_token2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476a06a7-7a31-4bdd-887f-fe8ac6c12e6b",
   "metadata": {},
   "source": [
    "As you can see, the effectiveness of stemming depends on the type of word. The more variations a word has, the harder it is for stemming to handle it (e.g., fly → flew → flown).\n",
    "\n",
    "Some of these problems can be fixed with a **lemmatizer**. A lemmatizer doesn't simply strip off letters but looks up verb tenses and takes into account the part of speech of each word. **Part of Speech (POS)** tagging allows us to see the parts of speech of various tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5879cc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Last', 'JJ'), ('weekend', 'NN'), (',', ','), ('I', 'PRP'), ('traveled', 'VBD'), ('to', 'TO'), ('New', 'NNP'), ('York', 'NNP'), ('City', 'NNP'), ('and', 'CC'), ('wrote', 'VBD'), ('about', 'IN'), ('my', 'PRP$'), ('experience', 'NN'), ('on', 'IN'), ('https', 'NN'), (':', ':'), ('//example.com', 'NN'), ('—', 'IN'), ('it', 'PRP'), ('was', 'VBD'), ('amazing', 'VBG'), ('!', '.'), ('#', '#'), ('TravelDiaries', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "# Part of Speech Tagging\n",
    "pos_list = nltk.pos_tag(nltk.word_tokenize(string))\n",
    "print(pos_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12961d94-a88e-4a7b-b3f2-a019579e706b",
   "metadata": {},
   "source": [
    "The above output shows a tag connected to each token (e.g. JJ → Adjective, NN → Noun singular, VBD → verb, past tense etc...).\n",
    "\n",
    "In the following example we can see a lemmatizer in action, compared to a stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38af77ed-766d-428a-b1fa-ed1bdbd3fdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\tStemmed\tLemmatized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Utente\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\trun\trun\n",
      "flies\tfli\tfly\n",
      "better\tbetter\tbetter\n",
      "studies\tstudi\tstudy\n",
      "mice\tmice\tmice\n",
      "geese\tgees\tgeese\n",
      "went\twent\tgo\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [\"running\", \"flies\", \"better\", \"studies\", \"mice\", \"geese\", \"went\"]\n",
    "\n",
    "print(\"Word\\tStemmed\\tLemmatized\")\n",
    "\n",
    "for word in words:\n",
    "    stemmed = stemmer.stem(word)\n",
    "    lemmatized = lemmatizer.lemmatize(word, pos='v')  # 'v' = verb\n",
    "    print(f\"{word}\\t{stemmed}\\t{lemmatized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50c392d-231a-436e-af86-cd432c740f92",
   "metadata": {},
   "source": [
    "It is clear here how the lemmatized version captures more nuances of the listed words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338f251e",
   "metadata": {},
   "source": [
    "## spaCy\n",
    "\n",
    "spaCy takes a different approach from NLTK, creating a document model of a text. It is more sophisticated, but uses a different syntax for NLP tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d78da23a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (25.1.1)\n",
      "Collecting pip\n",
      "  Using cached pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (80.9.0)\n",
      "Collecting wheel\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Using cached pip-25.3-py3-none-any.whl (1.8 MB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: To modify pip, please run the following command:\n",
      "C:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python313\\python.exe -m pip install -U pip setuptools wheel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (0.16.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (2.3.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (2.32.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.7.14)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.1.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\utente\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 1.0/12.8 MB 7.2 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 2.1/12.8 MB 6.5 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 6.2 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 4.5/12.8 MB 5.8 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 4.5/12.8 MB 5.8 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 6.0/12.8 MB 5.0 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 5.0 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.7/12.8 MB 5.3 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.0/12.8 MB 5.5 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.7/12.8 MB 5.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.1/12.8 MB 5.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 5.3 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install the spaCy Program\n",
    "# For installation, see https://spacy.io/usage\n",
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef85c18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Last', 'weekend', ',', 'I', 'traveled', 'to', 'New', 'York', 'City', 'and', 'wrote', 'about', 'it', 'on', 'https://example.com', '#', 'TravelDiaries']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "string = \"Last weekend, I traveled to New York City and wrote about it on https://example.com #TravelDiaries\"\n",
    "\n",
    "my_doc = nlp(string)\n",
    "\n",
    "tokens = []\n",
    "for token in my_doc:\n",
    "    tokens.append(token.text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed68a265",
   "metadata": {},
   "source": [
    "If we want to improve on the spaCy default tokenization, it is possible to add [add rules](https://machinelearningknowledge.ai/complete-guide-to-spacy-tokenizer-with-examples/). In the example in the above we could tokenize togheter the New York City string and the #TravalDiaries. We can do this in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a9d79bf-7d27-49be-862d-8052b28b349e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Last', 'weekend', ',', 'I', 'traveled', 'to', 'New York City', 'and', 'wrote', 'about', 'it', 'on', 'https://example.com', '#TravelDiaries']\n"
     ]
    }
   ],
   "source": [
    "from spacy.symbols import ORTH\n",
    "\n",
    "# Add a special case for \"New York City\"\n",
    "special_case = [{\"ORTH\": \"New York City\"}]\n",
    "nlp.tokenizer.add_special_case(\"New York City\", special_case)\n",
    "\n",
    "# Add a special case for \"#TravelDiaries\"\n",
    "special_case2 = [{\"ORTH\": \"#TravelDiaries\"}]\n",
    "nlp.tokenizer.add_special_case(\"#TravelDiaries\", special_case2)\n",
    "\n",
    "my_doc = nlp(string)\n",
    "\n",
    "tokens = []\n",
    "for token in my_doc:\n",
    "    tokens.append(token.text)\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c2bd63-f41d-4242-bb69-4cfa8069d605",
   "metadata": {},
   "source": [
    "spaCy also supports Parts of Speech tagging and lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9d96ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parts of Speech\n",
      "Last ADJ\n",
      "weekend NOUN\n",
      ", PUNCT\n",
      "I PRON\n",
      "traveled VERB\n",
      "to ADP\n",
      "New PROPN\n",
      "York PROPN\n",
      "City PROPN\n",
      "and CCONJ\n",
      "wrote VERB\n",
      "about ADP\n",
      "it PRON\n",
      "on ADP\n",
      "https://example.com X\n",
      "# SYM\n",
      "TravelDiaries PROPN\n",
      "\n",
      "Lemmatizations\n",
      "Last last\n",
      "weekend weekend\n",
      ", ,\n",
      "I I\n",
      "traveled travel\n",
      "to to\n",
      "New New\n",
      "York York\n",
      "City City\n",
      "and and\n",
      "wrote write\n",
      "about about\n",
      "it it\n",
      "on on\n",
      "https://example.com https://example.com\n",
      "# #\n",
      "TravelDiaries TravelDiaries\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "my_doc = nlp(string)\n",
    "\n",
    "print('Parts of Speech')\n",
    "for token in my_doc:\n",
    "    print(token, token.pos_,)\n",
    "\n",
    "print('\\nLemmatizations')\n",
    "for token in my_doc:\n",
    "    print(token, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376d2ac9",
   "metadata": {},
   "source": [
    "We can gather our n-grams by defining a function that accepts our tokens and an argument `n` for the \"n\" in \"n-gram.\" So, a bigram would be n = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4374a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for gathering n-grams with spaCy\n",
    "def n_grams(tokens, n):\n",
    "    n_grams = []\n",
    "    for i in range(len(tokens)-n+1):\n",
    "        n_grams.append(tokens[i:i+n])\n",
    "    return(n_grams)\n",
    "    # return[tokens[i:i+n] for i in range(len(tokens)-n+1)] # Written as a list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e492a401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Last', 'weekend'], ['weekend', ','], [',', 'I'], ['I', 'traveled'], ['traveled', 'to'], ['to', 'New'], ['New', 'York'], ['York', 'City'], ['City', 'and'], ['and', 'wrote'], ['wrote', 'about'], ['about', 'it'], ['it', 'on'], ['on', 'https://example.com'], ['https://example.com', '#'], ['#', 'TravelDiaries']]\n",
      "[['Last', 'weekend', ','], ['weekend', ',', 'I'], [',', 'I', 'traveled'], ['I', 'traveled', 'to'], ['traveled', 'to', 'New'], ['to', 'New', 'York'], ['New', 'York', 'City'], ['York', 'City', 'and'], ['City', 'and', 'wrote'], ['and', 'wrote', 'about'], ['wrote', 'about', 'it'], ['about', 'it', 'on'], ['it', 'on', 'https://example.com'], ['on', 'https://example.com', '#'], ['https://example.com', '#', 'TravelDiaries']]\n"
     ]
    }
   ],
   "source": [
    "bigrams = n_grams(tokens, 2)\n",
    "trigrams = n_grams(tokens, 3)\n",
    "print(bigrams)\n",
    "print(trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f5265f",
   "metadata": {},
   "source": [
    "While NLTK and spaCy tokenizers are the most prominent, there are also tokenizers available for packages such as:\n",
    "\n",
    "* [Gensim](https://radimrehurek.com/gensim/)\n",
    "* [Keras](https://keras.io/)\n",
    "* [Stanford NLP](https://nlp.stanford.edu/software/tokenizer.shtml)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
